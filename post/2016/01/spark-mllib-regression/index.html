<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=0.9"><title>Sparkで機械学習： 回帰モデルで値を予測する - Namiking.net</title><link rel="stylesheet" href="/css/bundle.css"><script src="/js/bundle.js"></script></head><body><div class="layout-sitenavi"><div class="container"><div class="layout-sitenavi__title"><a href="http://blog.namiking.net/"><strong>Namiking.net</strong><p>Web系エンジニアの技術ブログ</p></a></div><div class="layout-sitenavi__menu"><ul><li><a href="http://blog.namiking.net/about/"><i class="fa fa-lg fa-user"></i><span>ABOUT</span></a></li><li><a href="https://github.com/namikingsoft/"><i class="fa fa-lg fa-github"></i><span>GITHUB</span></a></li><li><a href="https://twitter.com/namikingsoft/"><i class="fa fa-lg fa-twitter"></i><span>TWITTER</span></a></li></ul></div></div></div><div class="layout-headline-single"><div class="container"><header><div class="layout-headline-single__title"><h1>Sparkで機械学習： 回帰モデルで値を予測する</h1></div><div class="lead"><div class="layout-headline-single__meta"><ul><li><i class="fa fa-calendar"></i>2016-01-31 (Sun)</li><li><img src="https://s.gravatar.com/avatar/3706c1a344dc2282c6683b6c6d0926f2?s=27&r=g">namikingsoft</li></ul></div><div class="layout-headline-single__taxonomies"><ul><li><a class="label label-success" href="/categories/spark%E3%81%A7%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">Sparkで機械学習</a></li><li><a class="label label-default" href="/tags/spark">Spark</a></li><li><a class="label label-default" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a></li><li><a class="label label-default" href="/tags/%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0">線形回帰</a></li><li><a class="label label-default" href="/tags/%E6%B1%BA%E5%AE%9A%E6%9C%A8%E5%9B%9E%E5%B8%B0">決定木回帰</a></li><li><a class="label label-default" href="/tags/zeppelin">Zeppelin</a></li></ul></div></div></header></div></div><div class="layout-content"><div class="container"><article><div class="layout-content__body"><div class="module-markdown">

<p>Apache Spark上にて、簡単なCSVのサンプルデータを取り込み、線形回帰や決定木回帰を利用して、穴が空いた項目を予測するサンプルプログラムを書いてみる。</p>

<h3 id="サンプルデータ-身体情報から結婚時期を予測する">サンプルデータ (身体情報から結婚時期を予測する)</h3>

<p>実データではありません。入門用にシンプルで法則性のあるデータを探したのですが、なかなか見つからなかったので、自分で訓練用のデータを作ってみた。</p>

<p>題材としては、性別や血液型、身長、体重から、結婚適齢期を予測するみたいなことをやってみる。例えば、以下の様なデータを学習用データとして取り込み、</p>

<table>
<thead>
<tr>
<th align="center">結婚した歳</th>
<th align="center">血液型</th>
<th align="center">性別</th>
<th align="right">身長(cm)</th>
<th align="right">体重(kg)</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">32歳</td>
<td align="center">O</td>
<td align="center">女</td>
<td align="right">152</td>
<td align="right">60</td>
</tr>

<tr>
<td align="center">42歳</td>
<td align="center">A</td>
<td align="center">男</td>
<td align="right">180</td>
<td align="right">80</td>
</tr>

<tr>
<td align="center">26歳</td>
<td align="center">O</td>
<td align="center">男</td>
<td align="right">155</td>
<td align="right">55</td>
</tr>

<tr>
<td align="center">20歳</td>
<td align="center">B</td>
<td align="center">女</td>
<td align="right">166</td>
<td align="right">55</td>
</tr>

<tr>
<td align="center">&hellip;</td>
<td align="center">&hellip;</td>
<td align="center">&hellip;</td>
<td align="right">&hellip;</td>
<td align="right">&hellip;</td>
</tr>
</tbody>
</table>

<p>以下の「？」になっているところの値を予測する、みたいなことをやってみる。</p>

<table>
<thead>
<tr>
<th align="center">結婚する歳</th>
<th align="center">血液型</th>
<th align="center">性別</th>
<th align="right">身長(cm)</th>
<th align="right">体重(kg)</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">？</td>
<td align="center">O</td>
<td align="center">女</td>
<td align="right">152</td>
<td align="right">60</td>
</tr>

<tr>
<td align="center">？</td>
<td align="center">A</td>
<td align="center">男</td>
<td align="right">180</td>
<td align="right">80</td>
</tr>
</tbody>
</table>

<blockquote>
<p>サンプルデータ： 身体情報から結婚時期を予測する<br />
<a href="/files/post/2016/01/spark-mllib-regression/training.csv">CSV形式のダウンロード</a></p>
</blockquote>

<h4 id="データの傾向">データの傾向</h4>

<p>ただ無差別にデータを作っても、予測が合ってるかどうかの判断がつかないため、<br />
以下の様な<strong>事実無根</strong>な法則で値をでっちあげてみた。</p>

<ul>
<li>B型は早婚</li>
<li>O型は晩婚</li>
<li>AB型はとても早婚</li>
<li>女性は早婚</li>
<li>肥満とモヤシは晩婚</li>
<li>男性の高身長はとても晩婚</li>
</ul>

<h3 id="コーディング前の準備">コーディング前の準備</h3>

<h4 id="apache-zeppelinのインストール">Apache Zeppelinのインストール</h4>

<p>Spark(ScalaやPython)の記述やその他細かいシェルスクリプトなどの操作をWeb上でインタラクティブに行えるノートブック系OSS<sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup>。この記事では、Sparkの操作は基本的にこのソフトを用いてコーディングを行っている。Sparkも一緒に含まれているので、これをローカルにインストールするだけで概ね動くはず。</p>

<blockquote>
<p>Apache Zeppelin (incubating)<br />
<a href="https://zeppelin.incubator.apache.org/">https://zeppelin.incubator.apache.org/</a></p>
</blockquote>

<h3 id="サンプルデータの取り込み">サンプルデータの取り込み</h3>

<p>ここからは、Zeppelin上での作業となる。事前にZeppelinを起動して、適当な新しいノートブックを作成しておく。</p>

<h4 id="サンプルデータのダウンロード">サンプルデータのダウンロード</h4>

<pre><code class="language-scala">%sh
curl -Lo /tmp/training.csv \
  &quot;http://blog.namiking.net/files/post/2016/01/spark-mllib-regression/training.csv&quot;
</code></pre>

<p>先ほどのCSV形式のサンプルデータを一時保存領域にダウンロードするシェルスクリプトを記述する。</p>

<h4 id="１レコード毎のcaseクラスを作成">１レコード毎のCaseクラスを作成</h4>

<pre><code class="language-scala">case class Profile(
  marriedAge: Option[Double],
  blood: String,
  sex: String,
  height: Double,
  weight: Double
)
</code></pre>

<p>CSVデータなどをDataFrame形式に変換する際に必要になる。
<code>marriedAge</code>をOption型にしているのは、テストデータを取り込む際に入れる値が無いため。</p>

<h4 id="csvをパースして-dataframe形式に変換">CSVをパースして、DataFrame形式に変換</h4>

<pre><code class="language-scala">var csvRDD = sc.textFile(&quot;/tmp/training.csv&quot;)
var csvHeadRDD = sc.parallelize(Array(csvRDD.first))
var training = csvRDD
  .subtract(csvHeadRDD) // ヘッダ除去
  .map { line =&gt;
    val cols = line.split(',')
    Profile(
      marriedAge = Some(cols(0).toDouble),
      blood = cols(1),
      sex = cols(2),
      height = cols(3).toDouble,
      weight = cols(4).toDouble
    )
  }.toDF
</code></pre>

<p>RDDだと、<code>tail</code>や<code>slice</code>関数みたいなものがなくて、ヘッダ除去程度でもちょっとまどろっこしい。分散処理を考えるとしかたないのだろうか。</p>

<h4 id="おまけ-spark-csvモジュールを使ったサンプルデータの取り込み">[おまけ] spark-csvモジュールを使ったサンプルデータの取り込み</h4>

<p>先ほどの手順で、サンプルデータの取り込みは完了だが、もう１パターン、CSVのパースやDataFrame変換を手伝ってくれるspark-csvモジュールを使ったCSV取り込みを書いておく。</p>

<blockquote>
<p>Zeppelinはパラグラフごとに実行するしないを制御できるので、別パターンのコードや使わなくなったコードも、そのまま残しておいても支障はない。後になって再利用できたりするので、残しておくと便利かも。</p>
</blockquote>

<h5 id="依存モジュールロード">依存モジュールロード</h5>

<pre><code class="language-scala">%dep
z.reset()
z.load(&quot;com.databricks:spark-csv_2.11:1.3.0&quot;)
</code></pre>

<p><code>%dep</code>(Dependency)については、Sparkが起動する前に行わないと、以下の様なエラーが出る。</p>

<pre><code>Must be used before SparkInterpreter (%spark) initialized
Hint: put this paragraph before any Spark code and restart Zeppelin/Interpreter
</code></pre>

<p>既に起動してしまっている場合は、Zeppelinを再起動するか、InterpreterページのSpark欄の<code>restart</code>ボタンを押下する。</p>

<h5 id="spark-csvを使ったcsvの取り込み">spark-csvを使ったCSVの取り込み</h5>

<pre><code class="language-scala">import org.apache.spark.sql.types.{
  StructType,
  StructField,
  StringType,
  DoubleType
}
val customSchema = StructType(Seq(
  StructField(&quot;marriedAge&quot;, DoubleType, true),
  StructField(&quot;blood&quot;, StringType, true),
  StructField(&quot;sex&quot;, StringType, true),
  StructField(&quot;height&quot;, DoubleType, true),
  StructField(&quot;weight&quot;, DoubleType, true)
))
val training = sqlContext.read
  .format(&quot;com.databricks.spark.csv&quot;)
  .option(&quot;header&quot;, &quot;true&quot;)
  .schema(customSchema)
  .load(&quot;/tmp/training.csv&quot;)
</code></pre>

<p>CSVのパースやヘッダ除去(項目名に利用)まで自動で行ってくれる。型指定まで自動で行うオプション(inferSchema)もあるが、Double型がInteger型になってしまったので、今回は手動で指定した。</p>

<h3 id="サンプルデータの前処理">サンプルデータの前処理</h3>

<p>Sparkの機械学習ライブラリでは、各分析アルゴリズムにデータを引き渡す前処理として、特徴データ(血液型、性別、身長、体重)を、まとめてベクトル形式に変換する必要がある。</p>

<p>そういった前処理を楽にするために、前処理や回帰モデル設定、訓練データ取り込みを一貫して行えるspark.mlのPipelineを利用してみる。Pipelineについては、<a href="https://spark.apache.org/docs/1.6.0/ml-guide.html">公式のドキュメント</a>が詳しい。別サイトに<a href="http://mogile.web.fc2.com/spark/ml-guide.html">日本語訳</a>もあった。</p>

<h4 id="文字列インデクサ">文字列インデクサ</h4>

<pre><code class="language-scala">import org.apache.spark.ml.feature.StringIndexer

val bloodIndexer = new StringIndexer()
  .setInputCol(&quot;blood&quot;)
  .setOutputCol(&quot;bloodIndex&quot;)
val sexIndexer = new StringIndexer()
  .setInputCol(&quot;sex&quot;)
  .setOutputCol(&quot;sexIndex&quot;)
</code></pre>

<p>StringIndexerはパイプラインをつなぐための部品の一つ。<code>A</code>,<code>B</code>,<code>O</code>,<code>AB</code>といった文字列のカテゴリデータを<code>0.0</code>,<code>1.0</code>,<code>2.0</code>,<code>3.0</code>みたいに実数のインデックスに変換してくれる。</p>

<h4 id="複数項目のベクトル化">複数項目のベクトル化</h4>

<pre><code class="language-scala">import org.apache.spark.ml.feature.VectorAssembler

val assembler = new VectorAssembler()
  .setInputCols(Array(
    &quot;bloodIndex&quot;,
    &quot;sexIndex&quot;,
    &quot;height&quot;,
    &quot;weight&quot;
  ))
  .setOutputCol(&quot;features&quot;)
</code></pre>

<p>VectorAssemblerはパイプラインをつなぐための部品の一つ。複数項目の実数データを一つの特徴ベクトルデータに変換してくれる。</p>

<h4 id="ベクトル標準化">ベクトル標準化</h4>

<pre><code class="language-scala">import org.apache.spark.ml.feature.StandardScaler

val scaler = new StandardScaler()
  .setInputCol(assembler.getOutputCol)
  .setOutputCol(&quot;scaledFeatures&quot;)
</code></pre>

<p>StandardScalerはパイプラインをつなぐための部品の一つ。基準が違うデータを取り込むと予測が不安定になるため、特徴ベクトルデータを標準化する。</p>

<p>これで前処理のためのパイプライン部品は揃った。</p>

<h3 id="線形回帰モデルの作成して-予測値を得る">線形回帰モデルの作成して、予測値を得る</h3>

<p>まずは線形回帰を用いて、値の予測を行うためのモデルを作成する。</p>

<h4 id="線形回帰">線形回帰</h4>

<pre><code class="language-scala">import org.apache.spark.ml.regression.LinearRegression

val regression = new LinearRegression()
  .setLabelCol(&quot;marriedAge&quot;)
  .setFeaturesCol(scaler.getOutputCol)
</code></pre>

<p>パイプライン部品の一つ。<code>setLabelCol</code>には、予想したい項目を指定する。</p>

<h4 id="パイプライン作成">パイプライン作成</h4>

<pre><code class="language-scala">import org.apache.spark.ml.Pipeline

val pipeline = new Pipeline()
  .setStages(Array(
    bloodIndexer,
    sexIndexer,
    assembler,
    scaler,
    regression
  ))
</code></pre>

<p>今までのパイプライン部品を繋げて、パイプラインを作成する。</p>

<h4 id="クロス検証でチューニング設定をして-モデルを作成">クロス検証でチューニング設定をして、モデルを作成</h4>

<pre><code class="language-scala">import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.tuning.{
  ParamGridBuilder,
  CrossValidator
}

val paramGrid = new ParamGridBuilder()
  .addGrid(regression.regParam, Array(0.1, 0.5, 0.01))
  .addGrid(regression.maxIter, Array(10, 100, 1000))
  .build()

val evaluator = new RegressionEvaluator()
  .setLabelCol(regression.getLabelCol)
  .setPredictionCol(regression.getPredictionCol)

val cross = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(evaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(3)

val model = cross.fit(training)
</code></pre>

<p>モデルの精度を検証するために、クロス検証の設定をする。以下のような、めんどくさいチューニング処理を自動で行ってくれる便利な機能。</p>

<ul>
<li><code>paramGrid</code>で設定した配列のチューニング値で、全組み合わせのモデルを作成する。</li>
<li>サンプルデータを訓練データと検証データに分けて、一番精度の高いモデルを選択する。</li>
</ul>

<p>最終的には<code>model</code>変数に最適なモデルが束縛される。</p>

<h5 id="ちなみに-クロス検証を行わない場合は">ちなみに、クロス検証を行わない場合は</h5>

<pre><code class="language-scala">val model = pipeline.fit(training)
</code></pre>

<p>パイプラインに直接学習データを突っ込む。</p>

<h4 id="テストデータ作成">テストデータ作成</h4>

<pre><code class="language-scala">var test = sc.parallelize(Seq(
  // A型標準体型男
  Profile(None, &quot;A&quot;, &quot;男&quot;, 170, 65),
  // B型標準体型男
  Profile(None, &quot;B&quot;, &quot;男&quot;, 170, 65),
  // O型標準体型男
  Profile(None, &quot;O&quot;, &quot;男&quot;, 170, 65),
  // AB型標準体型男
  Profile(None, &quot;AB&quot;, &quot;男&quot;, 170, 65),
  // A型標準体型女
  Profile(None, &quot;A&quot;, &quot;女&quot;, 160, 50),
  // B型標準体型女
  Profile(None, &quot;B&quot;, &quot;女&quot;, 160, 50),
  // O型標準体型女
  Profile(None, &quot;O&quot;, &quot;女&quot;, 160, 50),
  // AB型標準体型女
  Profile(None, &quot;AB&quot;, &quot;女&quot;, 160, 50),
  // A型もやし男
  Profile(None, &quot;A&quot;, &quot;男&quot;, 170, 35),
  // A型でぶ男
  Profile(None, &quot;A&quot;, &quot;男&quot;, 170, 100),
  // A型もやし女
  Profile(None, &quot;A&quot;, &quot;女&quot;, 170, 35),
  // A型でぶ女
  Profile(None, &quot;A&quot;, &quot;女&quot;, 170, 100),
  // A型高身長男
  Profile(None, &quot;A&quot;, &quot;男&quot;, 190, 80),
  // A型小人(男)
  Profile(None, &quot;A&quot;, &quot;男&quot;, 17, 6),
  // A型巨人(男)
  Profile(None, &quot;A&quot;, &quot;男&quot;, 17000, 6500)
)).toDF
</code></pre>

<p>学習データの特徴データから大幅に外れるデータの予測も下の方に入れてみた。</p>

<h4 id="モデルから予測値を得る">モデルから予測値を得る</h4>

<pre><code class="language-scala">model.transform(test)
  .select(&quot;blood&quot;, &quot;sex&quot;, &quot;height&quot;, &quot;weight&quot;, &quot;prediction&quot;).show
</code></pre>

<pre><code>+-----+---+-------+------+------------------+
|blood|sex| height|weight|        prediction|
+-----+---+-------+------+------------------+
|    A|  男|  170.0|  65.0| 32.79763046781005|
|    B|  男|  170.0|  65.0|32.810260236687924|
|    O|  男|  170.0|  65.0|   32.803945352249|
|   AB|  男|  170.0|  65.0| 32.79131558337113|
|    A|  女|  160.0|  50.0|  28.7197777975515|
|    B|  女|  160.0|  50.0|28.732407566429345|
|    O|  女|  160.0|  50.0|28.726092681990423|
|   AB|  女|  160.0|  50.0| 28.71346291311255|
|    A|  男|  170.0|  35.0|36.194018649003766|
|    A|  男|  170.0| 100.0|28.835177589750728|
|    A|  女|  170.0|  35.0| 36.18913457728998|
|    A|  女|  170.0| 100.0| 28.83029351803694|
|    A|  男|  190.0|  80.0|  42.6417617554965|
|    A|  男|   17.0|   6.0|-48.82159525304273|
|    A|  男|17000.0|6500.0|  9017.13917142714|
+-----+---+-------+------+------------------+
</code></pre>

<h5 id="結果考察">結果考察</h5>

<ul>
<li>男性/女性の結婚時期の違いは、うまいこと現れた。</li>
<li>血液型による違いが、ほとんど現れなかった。</li>
<li>肥満/標準/もやしの違いは、よくわからない。</li>
<li>男性高身長のルールはうまく反映されている。</li>
</ul>

<p>ちなみに、南くんの恋人は生まれる50年前に結婚しており、巨神兵は結婚までに100世紀かかるらしい。突拍子もない結果に見えるが、<code>身長が高いほど結婚が遅い</code>というルールを線形的に捉えてくれているようにも見える。</p>

<h3 id="決定木回帰モデルを作成して-予測値を得る">決定木回帰モデルを作成して、予測値を得る</h3>

<p>決定木はクラス分類が得意な手法なので、今回のような細かいルールの設定でも、うまく予測してくれるかもしれない。</p>

<p>解析手法を変わるとはいえ、インタフェースが変わるわけではないので、
大幅にコードを変える必要はなく、試行錯誤が楽。</p>

<h4 id="クロス検証からモデルの作成まで">クロス検証からモデルの作成まで</h4>

<pre><code class="language-scala">import org.apache.spark.ml.regression.DecisionTreeRegressor
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.tuning.{
  ParamGridBuilder,
  CrossValidator
}

val regression = new DecisionTreeRegressor()
  .setLabelCol(&quot;marriedAge&quot;)
  .setFeaturesCol(scaler.getOutputCol)

val pipeline = new Pipeline()
  .setStages(Array(
    bloodIndexer,
    sexIndexer,
    assembler,
    scaler,
    regression
  ))

val paramGrid = new ParamGridBuilder()
  .addGrid(regression.maxBins, Array(2, 3, 4))
  .addGrid(regression.maxDepth, Array(10, 20, 30))
  .build()

val evaluator = new RegressionEvaluator()
  .setLabelCol(regression.getLabelCol)
  .setPredictionCol(regression.getPredictionCol)

val cross = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(evaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(3)

val model = cross.fit(training)
</code></pre>

<p>Regressorクラスの種類、paramGridで設定する項目が変わるぐらいで、その他は線形回帰のコードと変わらない。</p>

<h5 id="評価実行">評価実行</h5>

<pre><code class="language-scala">model.transform(test)
  .select(&quot;blood&quot;, &quot;sex&quot;, &quot;height&quot;, &quot;weight&quot;, &quot;prediction&quot;).show
</code></pre>

<pre><code>+-----+---+-------+------+------------------+
|blood|sex| height|weight|        prediction|
+-----+---+-------+------+------------------+
|    A|  男|  170.0|  65.0|41.666666666666664|
|    B|  男|  170.0|  65.0|22.857142857142858|
|    O|  男|  170.0|  65.0|              47.0|
|   AB|  男|  170.0|  65.0|19.666666666666668|
|    A|  女|  160.0|  50.0|              34.0|
|    B|  女|  160.0|  50.0|              20.0|
|    O|  女|  160.0|  50.0|              29.5|
|   AB|  女|  160.0|  50.0|              19.0|
|    A|  男|  170.0|  35.0|              35.0|
|    A|  男|  170.0| 100.0|41.666666666666664|
|    A|  女|  170.0|  35.0|              35.0|
|    A|  女|  170.0| 100.0|              41.0|
|    A|  男|  190.0|  80.0|              44.0|
|    A|  男|   17.0|   6.0|29.333333333333332|
|    A|  男|17000.0|6500.0|              44.0|
+-----+---+-------+------+------------------+
</code></pre>

<h5 id="結果考察-1">結果考察</h5>

<ul>
<li>どのルールも学習データの値に近い形で、うまいこと予測された。</li>
<li>ただ、下２つの人外データに関しても、学習データに近い値が出てしまっているので、学習データの特徴値から大きくハズレるレコードの予測値は大味になってしまう？</li>
</ul>

<h3 id="まとめ">まとめ</h3>

<p>今回は、CSV形式のサンプルデータを線形回帰と決定木回帰を用いて、値の予測を行った。</p>

<h5 id="線形回帰-1">線形回帰</h5>

<p>細かいルールまでは予測しきれなかったが、学習データにない特徴を持つデータでも、うまく特徴を捉えようと、努力していた感があった。</p>

<h5 id="決定木回帰">決定木回帰</h5>

<p>細かいルールに基づいた値をうまく予測してくれていたが、学習データにない特徴を持つデータに関しては、諦めていた感があった。</p>

<p>分析手法によって、予測結果の傾向が変わることを確認できた。今後、ランダムフォレスト回帰、勾配ブースト木回帰、生存回帰など色々な手法も試してみたい。</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">類似のOSSに、Jupyter(iPython Notebook)やspark-notebookがある。データの加工やモデルのチューニングで試行錯誤することが多いので、こういうソフトはかなり重宝する。
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
</ol>
</div>
</div></div><div class="layout-content__comment"><div class="module-comment"><div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'namikingsoft';
    var disqus_identifier = 'http:\/\/blog.namiking.net\/post\/2016\/01\/spark-mllib-regression\/';
    var disqus_title = 'Sparkで機械学習： 回帰モデルで値を予測する';
    var disqus_url = 'http:\/\/blog.namiking.net\/post\/2016\/01\/spark-mllib-regression\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></div></div></article></div></div><div class="layout-pagenavi"><div class="container"><div class="layout-pagenavi__relate"><div class="module-relate"></div></div><div class="layout-pagenavi__paging"><div class="module-paging"><div class="module-paging__prev col-xs-6"><a href="/post/2016/02/react-server-using-webpack/" title="サーバーサイドReactをwebpackを使って最小構成で試す (ES6 ＆ TypeScript)"><i class="fa fa-arrow-circle-o-left"></i><p class="hidden-xs">2016-02-13 (Sat)</p></a></div><div class="module-paging__next col-xs-6"><a href="/post/2016/01/docker-swarm-over-vpn/" title="クラウドとローカルをVPNでガッチャンコしたDockerネットワークを組んでみる"><i class="fa fa-arrow-circle-o-right"></i><p class="hidden-xs">2016-01-22 (Fri)</p></a></div></div></div></div></div><div class="layout-sharenavi"><div class="container"><h4>この記事について</h4><div class="layout-sharenavi__profile"><div class="module-profile"><div class="module-profile__avatar"><img src="https://s.gravatar.com/avatar/3706c1a344dc2282c6683b6c6d0926f2?s=80&r=g"><small>書いた人<br>Written by</small></div><div class="module-profile__text"><h3>namikingsoft</h3><p>何かを残して逝きたい<br>フロントエンドエンジニア</p><ul><li><a href="#"><i class="fa fa-2x fa-user"></i></a></li><li><a href="#"><i class="fa fa-2x fa-github"></i></a></li><li><a href="#"><i class="fa fa-2x fa-twitter"></i></a></li></ul></div></div></div><div class="layout-sharenavi__sharelink"><div class="module-sharelink"><ul class="module-sharelink__list"><li class="module-sharelink__list-twitter"><a class="twitter-share-button" data-url="http://blog.namiking.net/post/2016/01/spark-mllib-regression/" href="https://twitter.com/share" data-lang="ja" data-count="vertical" data-dnt="true">ツイート</a></li><li class="module-sharelink__list-facebook"><div class="fb-like" data-href="http://blog.namiking.net/post/2016/01/spark-mllib-regression/" data-layout="box_count" data-action="like" data-show-faces="true" data-share="false"></div></li><li class="module-sharelink__list-google"><div class="g-plusone" data-href="http://blog.namiking.net/post/2016/01/spark-mllib-regression/" data-size="tall"></div></li><li class="module-sharelink__list-hatena"><a class="hatena-bookmark-button" href="http://b.hatena.ne.jp/entry/http://blog.namiking.net/post/2016/01/spark-mllib-regression/" data-hatena-bookmark-layout="vertical-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border:none;"></a></li><li class="module-sharelink__list-pocket"><a class="pocket-btn" data-save-url="http://blog.namiking.net/post/2016/01/spark-mllib-regression/" data-pocket-label="pocket" data-pocket-count="vertical" data-lang="en"></a></li><li class="module-sharelink__list-line"><a href="http://line.me/R/msg/text/?http%3a%2f%2fblog.namiking.net%2fpost%2f2016%2f01%2fspark-mllib-regression%2f"><img src="/images/button/linebutton_36x60.png" width="36" height="60" alt="LINEに送る"></a></li></ul><div id="fb-root"></div></div></div></div></div><div class="layout-footer"><div class="container"><div class="layout-footer__poweredby col-sm-4"><div class="module-poweredby"><div class="module-poweredby__title"><a href="http://blog.namiking.net/"><strong>Namiking.net</strong><p>Web系エンジニアの技術ブログ</p></a></div><div class="module-poweredby__menu"><ul><li><a href="http://blog.namiking.net/"><i class="fa fa-lg fa-home"></i><span>HOME</span></a></li><li><a href="http://blog.namiking.net/about/"><i class="fa fa-lg fa-user"></i><span>ABOUT</span></a></li><li><a href="https://github.com/namikingsoft/"><i class="fa fa-lg fa-github"></i><span>GITHUB</span></a></li><li><a href="https://twitter.com/namikingsoft/"><i class="fa fa-lg fa-twitter"></i><span>TWITTER</span></a></li><li><a href="http://blog.namiking.net/index.xml"><i class="fa fa-lg fa-rss"></i><span>RSS</span></a></li></ul></div><div class="module-poweredby__hugo"><p>Designed &amp; Written by <a href="/about">@namikingsoft</a></p><p>Powered by <a href="http://gohugo.io">HUGO</a></p></div></div></div><div class="layout-footer__recent col-sm-4"><div class="module-recent"><h3>Recent Post</h3><dl><dt>2016-05-12 (Thu)</dt><dd><a href="http://blog.namiking.net/post/2016/05/flow-disadvantage/">静的型チェッカーflowを使ってみて、微妙に気になったこと４つ</a></dd><dt>2016-02-13 (Sat)</dt><dd><a href="http://blog.namiking.net/post/2016/02/react-server-using-webpack/">サーバーサイドReactをwebpackを使って最小構成で試す (ES6 ＆ TypeScript)</a></dd><dt>2016-01-31 (Sun)</dt><dd><a href="http://blog.namiking.net/post/2016/01/spark-mllib-regression/">Sparkで機械学習： 回帰モデルで値を予測する</a></dd><dt>2016-01-22 (Fri)</dt><dd><a href="http://blog.namiking.net/post/2016/01/docker-swarm-over-vpn/">クラウドとローカルをVPNでガッチャンコしたDockerネットワークを組んでみる</a></dd><dt>2016-01-18 (Mon)</dt><dd><a href="http://blog.namiking.net/post/2016/01/docker-swarm-build-using-tls/">TLS認証なDocker Swarmクラスタを構築 (docker-machineなしで)</a></dd></dl></div></div><div class="layout-footer__tagcloud col-sm-4"><div class="module-tagcloud"><h3>Tags</h3><ul><li><a href="/tags/digitalocean">digitalocean(1)</a></li><li><a href="/tags/docker">docker(9)</a></li><li><a href="/tags/docker-compose">docker-compose(1)</a></li><li><a href="/tags/ecmascript">ecmascript(2)</a></li><li><a href="/tags/eslint">eslint(1)</a></li><li><a href="/tags/express">express(1)</a></li><li><a href="/tags/flow">flow(1)</a></li><li><a href="/tags/flowtype">flowtype(1)</a></li><li><a href="/tags/javascript">javascript(1)</a></li><li><a href="/tags/libreboard">libreboard(1)</a></li><li><a href="/tags/mocha">mocha(1)</a></li><li><a href="/tags/oss">oss(4)</a></li><li><a href="/tags/react">react(1)</a></li><li><a href="/tags/restyaboard">restyaboard(2)</a></li><li><a href="/tags/softether">softether(1)</a></li><li><a href="/tags/spark">spark(2)</a></li><li><a href="/tags/swarm">swarm(4)</a></li><li><a href="/tags/taiga">taiga(2)</a></li><li><a href="/tags/terraform">terraform(1)</a></li><li><a href="/tags/tls">tls(1)</a></li><li><a href="/tags/typescript">typescript(1)</a></li><li><a href="/tags/vpn">vpn(1)</a></li><li><a href="/tags/webpack">webpack(2)</a></li><li><a href="/tags/webpack-dev-server">webpack-dev-server(1)</a></li><li><a href="/tags/wekan">wekan(2)</a></li><li><a href="/tags/zeppelin">zeppelin(1)</a></li><li><a href="/tags/%E3%82%A4%E3%83%B3%E3%83%95%E3%83%A9">インフラ(2)</a></li><li><a href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習(1)</a></li><li><a href="/tags/%E6%B1%BA%E5%AE%9A%E6%9C%A8%E5%9B%9E%E5%B8%B0">決定木回帰(1)</a></li><li><a href="/tags/%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0">線形回帰(1)</a></li></ul></div></div></div></div></body></html>